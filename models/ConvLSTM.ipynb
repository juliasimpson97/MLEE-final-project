{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87433041-c3fd-4889-8d43-e339d6a40861",
   "metadata": {},
   "source": [
    "# Following ConvLSTM_tf.ipynb from Shaun Kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25997b15-9938-452c-a3d4-ba953e72a6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 15:04:53.783940: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as kb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def custom_rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    custom_rmse(y_true, y_pred)\n",
    "    calculates root square mean value with focusing only on the ocean\n",
    "    \"\"\"\n",
    "    y_pred = y_pred[(y_true != 0) & (y_true != 0.0)]\n",
    "    y_true = y_true[(y_true != 0) & (y_true != 0.0)]\n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    return K.sqrt(K.mean(tf.math.squared_difference(y_pred, y_true),axis= -1))\n",
    "\n",
    "\n",
    "def eliminate_zero_pco2(pco2,socat=True):\n",
    "    if socat:\n",
    "        tmp=np.array(pco2.pCO2_socat.data)\n",
    "    else:\n",
    "        tmp=np.array(pco2.pCO2.data)\n",
    "        \n",
    "    ind=[]\n",
    "    \n",
    "    for i in range(421):\n",
    "        ind.append(np.nanmax(tmp[i]) != 0)\n",
    "    \n",
    "    return ind,tmp[ind]\n",
    "\n",
    "def inverse_scale_image(arr, df):\n",
    "    \"\"\"\n",
    "    inverse_scale_image(arr, df):\n",
    "    - inverses the pco2 scaling\n",
    "    \"\"\"\n",
    "    \n",
    "    old_min = np.nanmin(df)\n",
    "    old_max = np.nanmax(df)\n",
    "\n",
    "    output = arr*(old_max-old_min)/255+old_min\n",
    "    return output\n",
    "\n",
    "def inverse_scale_image_nfp(arr, df):\n",
    "    \"\"\"\n",
    "    inverse_scale_image(arr, df):\n",
    "    - inverses the pco2 scaling\n",
    "    \"\"\"\n",
    "    \n",
    "    old_min = np.nanmin(df)\n",
    "    old_max = np.nanmax(df)\n",
    "\n",
    "    y_pred = arr*(old_max-old_min)/255+old_min\n",
    "    \n",
    "    tmp=np.nan_to_num(pco2.pCO2.data[X_index][1:])\n",
    "    y_true=np.expand_dims(tmp,axis=4)\n",
    "    y_pred[y_true==0]=0\n",
    "    return y_true,y_pred\n",
    "\n",
    "def get_point_prediction(pred,lon,lan):\n",
    "    pco2_value = pred[lan][lon]\n",
    "    return pco2_value\n",
    "\n",
    "\n",
    "def df_to_xarray(df_in=None):\n",
    "    '''\n",
    "    df_to_xarray(df_in) converts dataframe to dataset\n",
    "        this makes a monthly 1x1 skeleton dataframe already\n",
    "        time, lat, lon need to be in the dataframe\n",
    "    !! this take 4 minutes !!\n",
    "    example\n",
    "    ==========\n",
    "    ds = df_to_xarray(df_in = df[['time','lat','lon','sst']])\n",
    "    '''\n",
    "    # to make date in attributes\n",
    "    from datetime import date\n",
    "    # Make skeleton \n",
    "    dates = pd.date_range(start=f'1982-01-01', end=f'2018-12-01',freq='MS') + np.timedelta64(14, 'D')\n",
    "    ds_skeleton = xr.Dataset({'lon':np.arange(0.5, 360, 1), \n",
    "                              'lat':np.arange(-89.5, 90, 1),\n",
    "                              'time':dates})    \n",
    "    # make dataframe\n",
    "    skeleton = ds_skeleton.to_dataframe().reset_index()[['time','lat','lon']]\n",
    "    # Merge predictions with df_all dataframe\n",
    "    try:\n",
    "        print(\"works\")\n",
    "        df_out = skeleton.merge(df_in, how = 'left', on = ['time','lat','lon'])\n",
    "    except:\n",
    "        print(\"maybe this\")\n",
    "        con=[skeleton,df_in]\n",
    "        df_out=pd.concat(con)\n",
    "        \n",
    "    # convert to xarray dataset\n",
    "    # old way to `dimt, = ds_skeleton.time.shape` ect. to get dimensions\n",
    "    # then reshape  `df_out.values.reshape(dim_lat, dim_lon, dim_time)`\n",
    "    # finally create a custom dataset\n",
    "    df_out.set_index(['time', 'lat','lon'], inplace=True)\n",
    "    ds = df_out.to_xarray()\n",
    "    #ds['sst'].attrs['units'] = 'uatm'\n",
    "    return ds\n",
    "\n",
    "def read_xarray(dir_name=\"\",num=\"001\",mpi=False,can=False):\n",
    "    '''\n",
    "     read_xarray(dir)name) opens data and returns data in xarray format for each feature\n",
    "    '''\n",
    "    date=\"198201-201701\"\n",
    "    file_type = \"CESM\"\n",
    "    if mpi:\n",
    "        file_type =\"MPI006\"\n",
    "        num=\"\"\n",
    "    elif can:\n",
    "        file_type = \"CanESM2r1r10\"\n",
    "        num=\"\"\n",
    "        date=\"198201-201712\"\n",
    "        \n",
    "    \n",
    "    chl = xr.open_dataset(f'{dir_name}/Chl_2D_mon_{file_type}{num}_1x1_{date}.nc')\n",
    "\n",
    "    mld = xr.open_dataset(f'{dir_name}/MLD_2D_mon_{file_type}{num}_1x1_{date}.nc')\n",
    "\n",
    "    sss = xr.open_dataset(f'{dir_name}/SSS_2D_mon_{file_type}{num}_1x1_{date}.nc')\n",
    "\n",
    "    sst = xr.open_dataset(f'{dir_name}/SST_2D_mon_{file_type}{num}_1x1_{date}.nc')\n",
    "\n",
    "    u10 = xr.open_dataset(f'{dir_name}/U10_2D_mon_{file_type}{num}_1x1_{date}.nc')\n",
    "\n",
    "    xco2 = xr.open_dataset(f'{dir_name}/XCO2_1D_mon_{file_type}{num}_native_198201-201701.nc')\n",
    "\n",
    "    icefrac = xr.open_dataset(f'{dir_name}/iceFrac_2D_mon_{file_type}{num}_1x1_{date}.nc')\n",
    "\n",
    "    patm = xr.open_dataset(f'{dir_name}/pATM_2D_mon_{file_type}{num}_1x1_{date}.nc')\n",
    "\n",
    "    pco2 = xr.open_dataset(f'{dir_name}/pCO2_2D_mon_{file_type}{num}_1x1_{date}.nc')\n",
    "\n",
    "    return chl,mld,sss,sst,u10,xco2,icefrac,patm,pco2\n",
    "\n",
    "\n",
    "\n",
    "def repeat_lat_and_lon(ds=None):\n",
    "    lon = np.arange(0.5,360,1)\n",
    "    lat = np.arange(-89.5,90,1)\n",
    "    ds_bc = xr.DataArray(np.zeros([len(lon),len(lat)]), coords=[('lon', lon),('lat', lat)])\n",
    "    ds_data, ds_mask = xr.broadcast(ds, ds_bc)\n",
    "    return ds_data\n",
    "\n",
    "def repeat_lon(ds=None):\n",
    "    lon = np.arange(0.5,360,1)\n",
    "    ds_bc = xr.DataArray(np.zeros([len(lon)]), coords=[('lon', lon)])\n",
    "    ds_data, ds_mask = xr.broadcast(ds, ds_bc)\n",
    "    return ds_data\n",
    "\n",
    "def repeat_lat(ds=None):\n",
    "    lat = np.arange(-89.5,90,1)\n",
    "    ds_bc = xr.DataArray(np.zeros([len(lat)]), coords=[('lat', lat)])\n",
    "    ds_data, ds_mask = xr.broadcast(ds, ds_bc)\n",
    "    return ds_data\n",
    "\n",
    "def repeat_time(ds=None, dates=None):\n",
    "    ''' dates needs to be a pandas date_range\n",
    "    Example\n",
    "    dates = pd.date_range(start='1982-01-01T00:00:00.000000000', \n",
    "                      end='2017-12-01T00:00:00.000000000',freq='MS')+ np.timedelta64(14, 'D')\n",
    "    '''\n",
    "    ds_bc = xr.DataArray(np.zeros([len(dates)]), coords=[('time', dates)])\n",
    "    ds_data, ds_mask = xr.broadcast(ds, ds_bc)\n",
    "    return ds_data\n",
    "\n",
    "def repeat_time_and_lon(ds=None, dates=None):\n",
    "    ''' dates needs to be a pandas date_range\n",
    "    Example\n",
    "    dates = pd.date_range(start='1998-01-01T00:00:00.000000000', \n",
    "                      end='2017-12-01T00:00:00.000000000',freq='MS')+ np.timedelta64(14, 'D')\n",
    "    '''\n",
    "    lon = np.arange(0.5,360,1)\n",
    "    ds_bc = xr.DataArray(np.zeros([len(dates), len(lon), ]), coords=[('time', dates),('lon', lon)])\n",
    "    ds_data, ds_mask = xr.broadcast(ds, ds_bc)\n",
    "    return ds_data\n",
    "\n",
    "def transform_doy(obj, dim='time'):\n",
    "    '''\n",
    "    transform_doy(ds, dim='time')\n",
    "    transform DOY into repeating cycles\n",
    "    \n",
    "    reference\n",
    "    ==========\n",
    "    Gregor et al. 2019 \n",
    "    '''\n",
    "    obj['T0'] = np.cos(obj[f'{dim}.dayofyear'] * 2 * np.pi / 365)\n",
    "    obj['T1'] = np.sin(obj[f'{dim}.dayofyear'] * 2 * np.pi / 365)\n",
    "    return obj[['T0','T1']]\n",
    "\n",
    "def compute_n_vector(obj, dim_lon='lon', dim_lat='lat'):\n",
    "    '''\n",
    "    compute_n_vector(ds,dim_lon='lon', dim_lat='lat')\n",
    "    calculate n-vector from lat/lon\n",
    "    \n",
    "    reference\n",
    "    ==========\n",
    "    Gregor et al. 2019 \n",
    "    '''\n",
    "    ### convert lat/lon to radians\n",
    "    obj['lat_rad'], obj['lon_rad'] = np.radians(obj[dim_lat]), np.radians(obj[dim_lon])\n",
    "\n",
    "    ### Calculate n-vector\n",
    "    obj['A'], obj['B'], obj['C'] = np.sin(obj['lat_rad']), \\\n",
    "                            np.sin(obj['lon_rad'])*np.cos(obj['lat_rad']), \\\n",
    "                            -np.cos(obj['lon_rad'])*np.cos(obj['lat_rad'])\n",
    "    return obj[['A','B','C']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db463b48-db1e-49fd-9309-041c55cc14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "#from utils import df_to_xarray,read_xarray\n",
    "\n",
    "def inverse_scale_frame(arr,df, X_index=[], socat=False):\n",
    "    \"\"\"\n",
    "    inverse_scale_frame(arr, df):\n",
    "    - inverses the pco2 scaling\n",
    "    \"\"\"\n",
    "    if len(X_index)==0: \n",
    "        X_index=np.lib.stride_tricks.sliding_window_view(range(421),3) \n",
    "    \n",
    "    if socat:\n",
    "        return inverse_scale_frame_socat(arr,df, X_index)\n",
    "\n",
    "\n",
    "    df_tmp = df[df!=0.0]\n",
    "    \n",
    "    old_min = np.nanmin(df_tmp)\n",
    "    old_max = np.nanmax(df_tmp)\n",
    "    y_pred = arr*(old_max-old_min)/255+old_min\n",
    "    \n",
    "    tmp=np.nan_to_num(df[X_index][1:])\n",
    "    y_true=np.expand_dims(tmp,axis=4)\n",
    "    y_pred[y_true==0]=0\n",
    "    return y_true,y_pred\n",
    "\n",
    "def inverse_scale_frame_socat(arr,df, X_index=[]):\n",
    "    \"\"\"\n",
    "    inverse_scale_frame(arr, df):\n",
    "    - inverses the pco2 scaling\n",
    "    \"\"\"\n",
    "    old_min = 0\n",
    "\n",
    "    df_tmp = df[df!=0.0]\n",
    "    old_max = np.nanmax(df_tmp)\n",
    "    y_pred = arr*(old_max-old_min)/255+old_min\n",
    "    tmp=np.nan_to_num(df[X_index][1:])\n",
    "    y_true=np.expand_dims(tmp,axis=4)\n",
    "    y_pred[y_true==0]=0\n",
    "    return y_true,y_pred\n",
    "\n",
    "\n",
    "def inverse_scale_image(arr, df, socat=False):\n",
    "    \"\"\"\n",
    "    inverse_scale_image(arr, df):\n",
    "    - inverses the pco2 scaling\n",
    "    \"\"\"\n",
    "    if socat:\n",
    "        return inverse_scale_image_socat(arr, df)\n",
    "    \n",
    "    old_min = np.nanmin(df)\n",
    "    old_max = np.nanmax(df)\n",
    "\n",
    "    y_pred = arr*(old_max-old_min)/255+old_min\n",
    "    \n",
    "    y_true=np.nan_to_num(df)\n",
    "    y_pred[y_true==0]=0\n",
    "    #y_true = np.expand_dims(y_true, axis=3)\n",
    "\n",
    "    return y_true,y_pred\n",
    "\n",
    "\n",
    "\n",
    "def inverse_scale_image_socat(arr, df):\n",
    "    \"\"\"\n",
    "    inverse_scale_image_socat(arr, df):\n",
    "    - inverses the pcPo2 scaling for socat\n",
    "    \"\"\"    \n",
    "    old_min = 0\n",
    "    old_max = np.nanmax(df)\n",
    "    y_pred = arr*(old_max-old_min)/255\n",
    "    \n",
    "    tmp=np.nan_to_num(df)\n",
    "    y_true = np.expand_dims(tmp, axis=3)\n",
    "    y_pred[y_true==0] = 0\n",
    "    return y_true,y_pred\n",
    "\n",
    "\n",
    "def preprocess_images(dir_name, num=\"001\",socat=False,mpi=False,can=False):\n",
    "    chl,mld,sss,sst,u10,xco2,icefrac,patm,pco2 = read_xarray(dir_name,num,mpi,can)\n",
    "    \n",
    "    if socat:\n",
    "\n",
    "        chl_images = preprocess_image_reduced(chl.Chl_socat.data)\n",
    "        mld_images = preprocess_image_reduced(mld.MLD_socat.data)\n",
    "        sss_images = preprocess_image_reduced(sss.SSS_socat.data)\n",
    "        sst_images = preprocess_image_reduced(sst.SST_socat.data)\n",
    "        xco2_images = preprocess_image_reduced(xco2.XCO2.data,xco2=True)\n",
    "        pco2_images = preprocess_image_reduced(pco2.pCO2_socat.data)\n",
    "    \n",
    "    else:\n",
    "        chl_images = preprocess_image_reduced(chl.Chl.data)\n",
    "        mld_images = preprocess_image_reduced(mld.MLD.data)\n",
    "        sss_images = preprocess_image_reduced(sss.SSS.data)\n",
    "        sst_images = preprocess_image_reduced(sst.SST.data)\n",
    "        xco2_images = preprocess_image_reduced(xco2.XCO2.data,xco2=True)\n",
    "        pco2_images = preprocess_image_reduced(pco2.pCO2.data)\n",
    "    \n",
    "    X = np.dstack((chl_images, mld_images, sss_images, sst_images, xco2_images))\n",
    "    X = X.reshape((421,180,360,5),order='F')\n",
    "\n",
    "    return X, pco2_images\n",
    "\n",
    "def preprocess_images_nfp(dir_name,num=\"001\",socat=False,mpi=False,can=False):\n",
    "    \n",
    "    chl,mld,sss,sst,u10,xco2,icefrac,patm,pco2 = read_xarray(dir_name,num,mpi,can)\n",
    "    \n",
    "    if socat:\n",
    "\n",
    "        chl_images = preprocess_image_reduced(chl.Chl_socat.data,socat=True)\n",
    "        mld_images = preprocess_image_reduced(mld.MLD_socat.data,socat=True)\n",
    "        sss_images = preprocess_image_reduced(sss.SSS_socat.data,socat=True)\n",
    "        sst_images = preprocess_image_reduced(sst.SST_socat.data,socat=True)\n",
    "        xco2_images = preprocess_image_reduced(xco2.XCO2.data,xco2=True)\n",
    "        pco2_images = preprocess_image_reduced(pco2.pCO2_socat.data,socat=True)\n",
    "    else:\n",
    "        chl_images = preprocess_image_reduced(chl.Chl.data)\n",
    "        mld_images = preprocess_image_reduced(mld.MLD.data)\n",
    "        sss_images = preprocess_image_reduced(sss.SSS.data)\n",
    "        sst_images = preprocess_image_reduced(sst.SST.data)\n",
    "        xco2_images = preprocess_image_reduced(xco2.XCO2.data,xco2=True)\n",
    "        pco2_images = preprocess_image_reduced(pco2.pCO2.data)\n",
    "    \n",
    "    X = np.dstack((chl_images, mld_images, sss_images, sst_images, xco2_images,pco2_images))\n",
    "    X = X.reshape((421,180,360,6),order='F')\n",
    "\n",
    "    return X, pco2_images\n",
    "\n",
    "\n",
    "def preprocess_image_reduced(data,xco2=False,socat=False):\n",
    "    if xco2:\n",
    "        return xco2_preprocess(data)\n",
    "    if socat :\n",
    "        return scale_image(convert_nan(data,socat=True))\n",
    "    \n",
    "    return scale_image(convert_nan(data))\n",
    "  \n",
    "\n",
    "### FOR SEQUENTIAL + VISION\n",
    "def create_shifted_frames(data):\n",
    "    x = data[:, 0 : data.shape[1] - 1, :, :]\n",
    "    y = data[:, 1 : data.shape[1], :, :]\n",
    "    return x, y\n",
    "\n",
    "### FOR VISION ###\n",
    "\n",
    "def xco2_preprocess(data):\n",
    "    \"\"\"\n",
    "    ## XCO2 Handling\n",
    "    # - xco2 values are a constant value across the globe, \n",
    "    # - creating an image layer with constant value for the model\n",
    "    # - xco2 layer improves prediction\n",
    "    \"\"\"\n",
    "    output=[]\n",
    "    min_xco2=np.min(data)\n",
    "    max_xco2=np.max(data)\n",
    "    new_min=0\n",
    "    new_max=255\n",
    "    \n",
    "    for i in data:\n",
    "        num = (i-min_xco2)*(new_max-new_min)/(max_xco2-min_xco2)+new_min\n",
    "        tmp = (np.repeat(num,180*360)).reshape(180,-1)\n",
    "        output.append(tmp)\n",
    "        \n",
    "    output=np.array(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def convert_nan(arr,socat=False):\n",
    "    \"\"\"\n",
    "    convert_nan(arr)\n",
    "    - converts nan values to the lowest value (continents)\n",
    "    \"\"\"\n",
    "    if socat:\n",
    "        nans=np.isnan(arr)\n",
    "        min_val=arr[~nans].min()\n",
    "        arr[nans]=min_val\n",
    "    else:\n",
    "        nans=np.isnan(arr)\n",
    "        min_val=arr[~nans].min()\n",
    "        arr[nans]=min_val-1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def add_dimension(arr):\n",
    "    \"\"\"\n",
    "    add_dimension(arr)\n",
    "    - add one dimension to axis=3\n",
    "    \"\"\"\n",
    "    images=np.expand_dims(arr, axis=3)\n",
    "    return images\n",
    "\n",
    "def scale_image(arr):\n",
    "    \"\"\"\n",
    "    scale_image(arr)\n",
    "    - scales numerical values from scale 0-255 for like an image\n",
    "    - have tried, regular normal/ min-max scaler -> does not work well\n",
    "    \"\"\"\n",
    "    ## Image Scale\n",
    "    min_pixel = arr.min() \n",
    "    #print(\"min:\",min_pixel)\n",
    "    max_pixel = arr.max()\n",
    "    #print(\"max:\",max_pixel)\n",
    "\n",
    "    new_min = 0\n",
    "    new_max = 255\n",
    "    arr = (arr-min_pixel)*(255)/(max_pixel-min_pixel)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960fb4dc-d7b9-47bf-a03f-07e966f7204d",
   "metadata": {},
   "source": [
    "Sources\n",
    "\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9063513\n",
    "\n",
    "https://keras.io/examples/vision/conv_lstm/\n",
    "\n",
    "https://github.com/sk981102/ocean_co2/blob/main/notebooks/deep_learning/002_ConvLSTM_tf.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f48f6b4-68e0-47b4-ab84-ef45900e7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "sys.path.insert(0, '../../src')\n",
    "\n",
    "#from utils_sk_example import df_to_xarray,read_xarray,inverse_scale_image, get_point_prediction #made utils_sk_example but normally just utils\n",
    "\n",
    "#sys.path.insert(0, '../../src/preprocess')\n",
    "#from data_preprocess_example import preprocess_image_reduced,preprocess_images_nfp  #made data_preprocess_example but normally just data_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d9a2b33-1187-47e3-b80d-83b1293e1142",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/home/julias/data/data1/Chl_2D_mon_CESM001_1x1_198201-201701.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/file_manager.py:201\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/lru_cache.py:55\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 55\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/julias/data/data1/Chl_2D_mon_CESM001_1x1_198201-201701.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m dir_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/data1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m val_dir_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/data2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m data,pco2 \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_images_nfp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m data_socat, pco2_socat \u001b[38;5;241m=\u001b[39m preprocess_images_nfp(dir_name, socat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m val_data,val_pco2 \u001b[38;5;241m=\u001b[39m preprocess_images_nfp(val_dir_name,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m035\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [4], line 112\u001b[0m, in \u001b[0;36mpreprocess_images_nfp\u001b[0;34m(dir_name, num, socat, mpi, can)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_images_nfp\u001b[39m(dir_name,num\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m001\u001b[39m\u001b[38;5;124m\"\u001b[39m,socat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,mpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,can\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 112\u001b[0m     chl,mld,sss,sst,u10,xco2,icefrac,patm,pco2 \u001b[38;5;241m=\u001b[39m \u001b[43mread_xarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m socat:\n\u001b[1;32m    116\u001b[0m         chl_images \u001b[38;5;241m=\u001b[39m preprocess_image_reduced(chl\u001b[38;5;241m.\u001b[39mChl_socat\u001b[38;5;241m.\u001b[39mdata,socat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [1], line 123\u001b[0m, in \u001b[0;36mread_xarray\u001b[0;34m(dir_name, num, mpi, can)\u001b[0m\n\u001b[1;32m    119\u001b[0m     num\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m198201-201712\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 123\u001b[0m chl \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdir_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/Chl_2D_mon_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_1x1_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m mld \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/MLD_2D_mon_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_1x1_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    127\u001b[0m sss \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/SSS_2D_mon_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_1x1_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/api.py:539\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    528\u001b[0m     decode_cf,\n\u001b[1;32m    529\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    538\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 539\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    546\u001b[0m     backend_ds,\n\u001b[1;32m    547\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    556\u001b[0m )\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:555\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    536\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    552\u001b[0m ):\n\u001b[1;32m    554\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 555\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:384\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    378\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    379\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    381\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    382\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    383\u001b[0m )\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:332\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:393\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:387\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    388\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/file_manager.py:189\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/.conda/envs/fromML/lib/python3.9/site-packages/xarray/backends/file_manager.py:207\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    205\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    206\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 207\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2463\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2026\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/home/julias/data/data1/Chl_2D_mon_CESM001_1x1_198201-201701.nc'"
     ]
    }
   ],
   "source": [
    "dir_name=\"../../data/data1\"\n",
    "val_dir_name=\"../../data/data2\"\n",
    "\n",
    "data,pco2 = preprocess_images_nfp(dir_name)\n",
    "data_socat, pco2_socat = preprocess_images_nfp(dir_name, socat = True)\n",
    "\n",
    "val_data,val_pco2 = preprocess_images_nfp(val_dir_name,\"035\")\n",
    "val_data_socat,val_pco2_socat = preprocess_images_nfp(val_dir_name,\"035\",socat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d05f2f-0376-455f-ae16-508920a89cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_index=np.lib.stride_tricks.sliding_window_view(range(421),3) \n",
    "\n",
    "y=np.expand_dims(pco2[X_index][1:],axis=4)\n",
    "X=data[X_index][:-1]\n",
    "\n",
    "val_y=np.expand_dims(val_pco2[X_index][1:],axis=4)\n",
    "val_X=val_data[X_index][:-1]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8910c9-35cd-4e2c-b5a3-60bb3ec57097",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y[0][0],cmap=\"RdBu\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597d57c-705c-434e-a4c6-02c1f71f7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE=X[0].shape\n",
    "OUTPUT_SHAPE=y[0].shape\n",
    "\n",
    "INPUT_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fade77d-9a5c-4c2a-af66-1e9a51fcaa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6d0d1-693e-469e-b3fc-b01f75964bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as kb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    custom_rmse(y_true, y_pred)\n",
    "    calculates root square mean value with focusing only on the ocean\n",
    "    \"\"\"\n",
    "    y_pred = y_pred[(y_true != 0) & (y_true != 0.0)]\n",
    "    y_true = y_true[(y_true != 0) & (y_true != 0.0)]\n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    return K.sqrt(K.mean(tf.math.squared_difference(y_pred, y_true),axis= -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac85413-a20e-4b61-b9c7-3f70faa02a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from functools import partial\n",
    "\n",
    "DefaultConvLSTM2D = partial(keras.layers.ConvLSTM2D,\n",
    "                        filters=32, kernel_size=(5, 5),\n",
    "                        padding=\"same\",return_sequences=True,\n",
    "                        activation=\"elu\",)\n",
    "\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConvLSTM2D(input_shape=INPUT_SHAPE),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    DefaultConvLSTM2D(kernel_size=(3,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    DefaultConvLSTM2D(kernel_size=(1,1)),\n",
    "    keras.layers.Conv3D(filters = 1, kernel_size=(3,3,3),activation=\"elu\", padding=\"same\")\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=custom_rmse, optimizer=keras.optimizers.Adam(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7700929-fbae-48a6-83d0-5979530c7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597b2ce-facd-485b-b6ad-11442ef491c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"../../models/base_CNN_LSTM_new.h5\"\n",
    "\n",
    "early_stoppings = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='min')\n",
    "checkpoint =  tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=0)\n",
    "callbacks=[early_stoppings,checkpoint]\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "# Fit the model to the training data.\n",
    "hist = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(val_X,val_y),\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0819858-c1f3-4462-856f-4fe57e91397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351bd44-8bf9-42d6-a7ab-810e8862119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model('../../models/base_CNN_LSTM_new.h5', custom_objects={'custom_rmse':custom_rmse})\n",
    "predicted_frames=best_model.predict(X,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bf8cb-0839-4c71-b05d-036a1a7099db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_frames[y==0]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c8bb4-0b0f-4b3e-9147-c31fc982ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "figure, axis = plt.subplots(2, 2,figsize=(12, 6))\n",
    "\n",
    "d = predicted_frames - y\n",
    "\n",
    "\n",
    "norm = mcolors.TwoSlopeNorm(vmin=d.min(), vmax = d.max(), vcenter=0)\n",
    "\n",
    "\n",
    "img=axis[0][0].imshow(np.flipud(predicted_frames[0][1]),cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "axis[0][0].set_title(\"prediction\")\n",
    "plt.colorbar(img,ax=axis)\n",
    "\n",
    "img1=axis[0][1].imshow(np.flipud(y[0][1]),cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "axis[0][1].set_title(\"true\")\n",
    "\n",
    "diff=np.flipud(np.squeeze(predicted_frames[0][1]-y[0][1]))\n",
    "img2=axis[1][0].imshow(diff,cmap=\"RdBu\", interpolation=\"nearest\",norm=norm)\n",
    "axis[1][0].set_title(\"residual\")\n",
    "plt.colorbar(img2,ax=axis)\n",
    "\n",
    "\n",
    "img2=axis[1][1].imshow(np.flipud(X[0][1][:,:,5]),cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "axis[1][1].set_title(\"input: previous pco2\")\n",
    "\n",
    "plt.savefig('../../assets/next-frame-prediction.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a091da-7c4e-48df-a682-5951a8a55845",
   "metadata": {},
   "source": [
    "# Creating Gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea158a1-ff4a-4ddc-84ab-7e222e5f828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "\n",
    "for i in range(418):\n",
    "    # plot the line chart\n",
    "    figure, axis = plt.subplots(2, 2,figsize=(12, 6))\n",
    "\n",
    "    img=axis[0][0].imshow(np.flipud(predicted_frames[i][1]),cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "    axis[0][0].set_title(\"prediction\")\n",
    "    plt.colorbar(img,ax=axis)\n",
    "\n",
    "    img1=axis[0][1].imshow(np.flipud(y[i][1]),cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "    axis[0][1].set_title(\"true\")\n",
    "\n",
    "    diff=np.flipud(np.squeeze(predicted_frames[i][1]-y[i][1]))\n",
    "    img2=axis[1][0].imshow(diff,cmap=\"RdBu\", interpolation=\"nearest\",norm=norm)\n",
    "    axis[1][0].set_title(\"residual\")\n",
    "    plt.colorbar(img2,ax=axis)\n",
    "    \n",
    "    img2=axis[1][1].imshow(np.flipud(X[i][1][:,:,5]),cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "    axis[1][1].set_title(\"input: previous pco2\")\n",
    "    # create file name and append it to a list\n",
    "    filename = f'{i}.png'\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    # save frame\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "with imageio.get_writer('../../assets/cnn-lstm.gif', mode='I') as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "        \n",
    "# Remove files\n",
    "for filename in set(filenames):\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b933dad-028c-4fd7-8af3-329178383afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = []\n",
    "\n",
    "for i in range(418):    \n",
    "    rmse = np.sqrt(np.mean((predicted_frames[i][1]-y[i][1])**2))\n",
    "    rmses.append(rmse)\n",
    "    \n",
    "plt.plot(rmses)\n",
    "plt.savefig('../../assets/nfp-overtime.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b56c0-4b0b-4c90-8b3e-e51e6ebf1551",
   "metadata": {},
   "source": [
    "# Getting pCO2 Prediction per Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de16e1b-67fe-4ee3-a388-80e9005a5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scale_image_nfp(arr, df):\n",
    "    \"\"\"\n",
    "    inverse_scale_image(arr, df):\n",
    "    - inverses the pco2 scaling\n",
    "    \"\"\"\n",
    "    \n",
    "    old_min = np.nanmin(df)\n",
    "    old_max = np.nanmax(df)\n",
    "\n",
    "    y_pred = arr*(old_max-old_min)/255+old_min\n",
    "    \n",
    "    tmp=np.nan_to_num(df[X_index][1:])\n",
    "    y_true=np.expand_dims(tmp,axis=4)\n",
    "    y_pred[y_true==0]=0\n",
    "    return y_true,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c88999-39ac-4d44-9bc3-312e26ffaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chl,mld,sss,sst,u10,fg_co2,xco2,icefrac,patm,tmp_pco2 = read_xarray(dir_name)\n",
    "y_true,y_pred=inverse_scale_image_nfp(predicted_frames,tmp_pco2.pCO2.data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e9f8c3-db7e-452b-89ed-279a290ac32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaled back RMSE score:\")\n",
    "\n",
    "np.sqrt(np.mean((y_true[:,:1]-y_pred[:,:1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c213608-80a8-4588-a163-deba99a938da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((y_true[:,:1]-y_pred[:,:1])**2)/np.sum(y_pred!=0.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fromML",
   "language": "python",
   "name": "fromml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
