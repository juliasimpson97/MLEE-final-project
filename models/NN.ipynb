{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00743f4-e104-4250-abb9-151a089163b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e0f753-5052-4522-bc07-40f3a98a9415",
   "metadata": {},
   "source": [
    "(to compare pre-transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10741bee-ac28-47e8-a90a-7a4191fb9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Machine learning libraries\n",
    "import sklearn            # machine-learning libary with many algorithms implemented\n",
    "#import xgboost as xgb     # extreme gradient boosting (XGB)\n",
    "#from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "# Python file with supporting functions\n",
    "import model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc6f61-11c2-4a81-9e0b-ae575dabc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_model_path = '/home/julias/MLEE-final-project/models/saved_models/recon_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467bbcd-f2f3-4023-b331-ea64c2da9554",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Split Datasets and Create Versions for Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fcffe-5a30-41ac-b254-cc70942693c6",
   "metadata": {},
   "source": [
    "## Load Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc0d1a-3d0e-4fad-9519-e3a06fe51edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/X.nc').to_dataframe().dropna()\n",
    "y_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/y.nc').to_dataframe().dropna() \n",
    "X_train_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/X_train.nc').to_dataframe().dropna() \n",
    "y_train_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/y_train.nc').to_dataframe().dropna()\n",
    "X_test_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/X_test.nc').to_dataframe().dropna() \n",
    "y_test_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/y_test.nc').to_dataframe().dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac04b13-20fd-43fd-b87f-5d2d3082fa84",
   "metadata": {},
   "source": [
    "Check that data was saved and loaded properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb89329f-e6a7-47be-98b2-47999abfbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4266d2-fc46-4189-9aab-f0f7c7fdf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba546f7-3a00-4c7a-b63d-8521d7b188c0",
   "metadata": {},
   "source": [
    "### Create Numpy Arrays for Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d9970-c39c-418b-ad4f-fd875a207756",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = X_df.to_numpy()         \n",
    "y_original = y_df.to_numpy().ravel() \n",
    "X_train_original = X_train_df.to_numpy() \n",
    "y_train_original = y_train_df.to_numpy().ravel()\n",
    "X_test_original = X_test_df.to_numpy()  \n",
    "y_test_original = y_test_df.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3184119-5595-4ac1-8982-c44a75113b6a",
   "metadata": {},
   "source": [
    "## Create Normalized Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98239f49-9280-4cf4-8b9a-4c7cf49664fd",
   "metadata": {},
   "source": [
    "*Note: Done here, as opposed to in test/train split, so that I can save the original train/test datasets and later determine whether normalization led to improvement. (My group has not historically normalized data before training.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecee45b-abdc-42e3-a85f-bb2f6500b2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_df_norm = (X_df - X_df.mean())/X_df.std() \n",
    "#y_df_norm = (y_df - y_df.mean())/y_df.std()\n",
    "#X_train_df_norm = (X_train_df - X_train_df.mean())/X_train_df.std()\n",
    "#y_train_df_norm = (y_train_df - y_train_df.mean())/y_train_df.std()\n",
    "#X_test_df_norm = (X_test_df - X_test_df.mean())/X_test_df.std()\n",
    "#y_test_df_norm = (y_test_df - y_test_df.mean())/y_test_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a44e58-fe0c-4e2b-ae5f-48f7ebd3e59b",
   "metadata": {},
   "source": [
    "CHANGE METHOD TO BELOW\n",
    "- This way, time and lat/lon conversions are not normalized\n",
    "- Note that coordinates don't seem to be input into ML algorithms, so T0,T1,A,B,C are the inputs of time and space (as would want, don't want two forms of time and space input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ff75f-4674-436d-8c3b-5583defcd3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_df_norm = X_df\n",
    "y_df_norm = y_df\n",
    "X_train_df_norm = X_train_df\n",
    "y_train_df_norm = y_train_df\n",
    "X_test_df_norm = X_test_df\n",
    "y_test_df_norm = y_test_df\n",
    "\n",
    "for df in X_df_norm, X_train_df_norm, X_test_df_norm:\n",
    "    df.SSS = (df.SSS - df.SSS.mean())/df.SSS.std() \n",
    "    df.SST = (df.SST - df.SST.mean())/df.SST.std() \n",
    "    df.MLD = (df.MLD - df.MLD.mean())/df.MLD.std() \n",
    "    df.Chl = (df.Chl - df.Chl.mean())/df.Chl.std() \n",
    "    df.XCO2 = (df.XCO2 - df.XCO2.mean())/df.XCO2.std() \n",
    "\n",
    "for df in y_df_norm, y_train_df_norm, y_test_df_norm:\n",
    "    df.pCO2 = (df.pCO2 - df.pCO2.mean())/df.pCO2.std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda086a0-3f93-48f8-a473-dfe7204f8e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d239bf1-164e-44cf-8b9f-e98c4e8f1905",
   "metadata": {},
   "source": [
    "## Create Numpy Arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e8d54-4cea-4f53-8ad2-6141f0f8d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n = X_df_norm.to_numpy()         \n",
    "y_n = y_df_norm.to_numpy().ravel() \n",
    "X_train_n = X_train_df_norm.to_numpy()\n",
    "y_train_n = y_train_df_norm.to_numpy().ravel()\n",
    "X_test_n = X_test_df_norm.to_numpy() \n",
    "y_test_n = y_test_df_norm.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de67bf9-7fdb-433b-8f49-04798e3232e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preliminary NN Model using Original (Not Normalized) Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e08329-b782-4318-847b-0234a2535769",
   "metadata": {},
   "source": [
    "## Build NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3285a-dfbc-4cfb-84fc-cd6059f7ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that number of input layer neurons must correspond to number of predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c7b34-a656-4973-8a0f-c50bddb5711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 64\n",
    "activation     = 'LeakyReLU'\n",
    "num_epochs     = 50\n",
    "learning_rate  = 0.001\n",
    "minibatch_size = 64\n",
    "model_num      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf97663-ae3c-4382-97d8-83bfe6515261",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d6bf4-9a85-4007-8aea-ef6af8541e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "NN_model.add(Dense(n_neuron, name='hidden_layer_1', activation=activation,input_shape=(X_train.shape[1],))) #  the 1st hidden layer \n",
    "NN_model.add(Dense(n_neuron, name='hidden_layer_2', activation=activation)) # the 2nd hidden layer\n",
    "NN_model.add(Dense(n_neuron, name='hidden_layer_3', activation=activation)) # the 3rd hidden layer\n",
    "NN_model.add(Dense(1, name='output_layer', activation='linear')) # the output layer\n",
    "\n",
    "\n",
    "NN_model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02822248-ca18-409d-93ff-bb2ffb7cfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e5ff7-d997-44eb-94ce-af0e37e51901",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "\n",
    "history = NN_model.fit(X_train_original, y_train_original, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5843abae-50f3-4b33-b9db-159af1f0a68e",
   "metadata": {},
   "source": [
    "## Initial Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee3774-5f25-4e80-81d9-c420444c052a",
   "metadata": {},
   "source": [
    "Can see that validation loss reaches minimums and jumps around, plot to see if trend indicates overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ef8f7-3b30-4721-86a5-445039ff4774",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce96e48-ff81-409c-abe2-ec5d06680489",
   "metadata": {},
   "source": [
    "Validation loss does jump around and is generally higher than the training loss, indicating that the model isn't generalizing well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa17830-260d-411e-a6e9-6045bb3f3e63",
   "metadata": {},
   "source": [
    "### Normalize so more comparable to other plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe75f2e-66a9-4bad-bfe8-77299a2bc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9771e-9445-4dc2-96aa-99e79af1dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_loss = (history.history['loss'] - np.mean(history.history['loss'])) / np.std(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9dddf-6b24-4a24-baeb-d5480e018d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_val_loss = (history.history['val_loss'] - np.mean(history.history['val_loss'])) / np.std(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23f6d5-334b-4e46-b430-d11bd79ce3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = history.epoch\n",
    "plt.plot(x_ax, norm_loss, label=\"training\")\n",
    "plt.plot(x_ax, norm_val_loss, label=\"validation\")\n",
    "plt.title(\"Normalized Training and Validation Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1d5c5-879f-48f2-a86e-6d67008a584c",
   "metadata": {},
   "source": [
    "## Save NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11fdfc-ff6a-406e-9e2c-d9f5dc602a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, save:\n",
    "NN_model.save(os.path.join(recon_model_path,'NN_model_oprelim_1.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a61ba-79d8-4b49-aad8-2ae339817221",
   "metadata": {},
   "source": [
    "## Test NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcbadfc-c85d-4ef8-bbaa-de43baf89406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then reload before start working with test data\n",
    "NN_prelim_model = load_model(os.path.join(recon_model_path,'NN_model_oprelim_1.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee887813-7c03-4130-a95b-87758a3e5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN_prelim_model.predict(X_test_df)\n",
    "mse = mean_squared_error(y_test_df, y_pred)\n",
    "print(\"MSE: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b17020-f0ed-4ded-abb4-054328118aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test_df))\n",
    "plt.plot(x_ax, y_test_df, label=\"original\")\n",
    "plt.plot(x_ax, y_pred, label=\"predicted\")\n",
    "plt.title(\"pC02 test and predicted data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0238b1-8c3a-46fb-8726-7d157ea07066",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preliminary NN Model using Normalized Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d07a232-3847-4085-ac2b-a8299dc52b8c",
   "metadata": {},
   "source": [
    "## Build NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84323fd7-e2d9-4157-8bea-14299df57876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 64\n",
    "activation     = 'LeakyReLU'\n",
    "num_epochs     = 50\n",
    "learning_rate  = 0.001\n",
    "minibatch_size = 64\n",
    "model_num      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ce14a-d99b-421b-aeb1-e4300596c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_nprelim = Sequential()\n",
    "\n",
    "NN_model_nprelim.add(Dense(n_neuron,  activation=activation,input_shape=(X_train.shape[1],))) #  the 1st hidden layer \n",
    "NN_model_nprelim.add(Dense(n_neuron,  activation=activation)) # the 2nd hidden layer\n",
    "NN_model_nprelim.add(Dense(n_neuron,  activation=activation)) # the 3rd hidden layer\n",
    "NN_model_nprelim.add(Dense(1,  activation='linear')) # the output layer\n",
    "\n",
    "\n",
    "NN_model_nprelim.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415335b8-85d1-466d-b8c1-eb1f7cd4edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_nprelim.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf7a2a-12a0-448c-8173-bdfc581669a4",
   "metadata": {},
   "source": [
    "## Train NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0bc27-0d98-433b-b157-248b247f3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "\n",
    "history_nprelim = NN_model_nprelim.fit(X_train, y_train, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d336b5-8489-40fc-b99f-2dbfa4e938c4",
   "metadata": {},
   "source": [
    "## Initial Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c720e9-2572-450e-9dbe-696a81987455",
   "metadata": {},
   "source": [
    "Can see that validation loss reaches minimums and jumps around, plot to see if trend indicates overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0311032-0eb4-417a-8fbc-6b635ea81613",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.plot_history(history_nprelim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2578c8-9ca8-4eba-8d62-f3e25d9081c1",
   "metadata": {},
   "source": [
    "While the training loss goes down substantially, the validation loss does not show the same trend. Instead, it remains (relatively) high and jumps around.\n",
    "- Completely fine, just signals that hyperparameter tuning will be needed (as expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1112fab-5c89-4825-9562-15567796ff30",
   "metadata": {},
   "source": [
    "### Normalize so more comparable to other plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fc9fa-fd07-42e9-b07a-5f3333651e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_nloss = (history_nprelim.history['loss'] - np.mean(history_nprelim.history['loss'])) / np.std(history_nprelim.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4943d4-f648-4520-a420-721662001e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_val_nloss = (history_nprelim.history['val_loss'] - np.mean(history_nprelim.history['val_loss'])) / np.std(history_nprelim.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d095a-8ef2-4893-971b-4505fc3f06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = history_nprelim.epoch\n",
    "plt.plot(x_ax, norm_nloss, label=\"training\")\n",
    "plt.plot(x_ax, norm_val_nloss, label=\"validation\")\n",
    "plt.title(\"Normalized Training and Validation Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be0e9f-eeaa-43cb-bb7a-c5b0db7e55d5",
   "metadata": {},
   "source": [
    "## Save NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8e74d-fc66-4cdf-aa39-04663b8b456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, save:\n",
    "NN_model_nprelim.save(os.path.join(recon_model_path,'NN_model_nprelim_1.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a12c36-5dc7-4419-846d-32f1eb6eff74",
   "metadata": {},
   "source": [
    "## Test NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0986fca-abd8-4005-bb0c-38d1d6fcc430",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_nprelim = load_model(os.path.join(recon_model_path,'NN_model_nprelim_1.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7b3e7-d287-4149-b802-390d562ce5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nprelim = NN_model_nprelim.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_nprelim_rerun)\n",
    "print(\"MSE: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6783f-40f5-4f2b-84fd-613147f0a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test))\n",
    "plt.plot(x_ax, y_test, label=\"original\")\n",
    "plt.plot(x_ax, y_pred_nprelim, label=\"predicted\")\n",
    "plt.title(\"pC02 test and predicted data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15331af5-9672-417c-be3c-0870d4707f1f",
   "metadata": {},
   "source": [
    "# Select Approach & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c304571-5689-466a-957e-29a6559d6b72",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65976d40-1aac-4452-9520-a8778d7df3d7",
   "metadata": {},
   "source": [
    "- Based on preliminary experimentation, will train the NN model on normalized data, as is standard machine learning practice\n",
    "- Comparison completed above for group reference, as data has not historically been normalized prior to training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606d841-9f96-4c1e-984c-f24508117dc3",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b4107-3bea-4c70-b399-1a0a3043f17b",
   "metadata": {},
   "source": [
    "Experimentation with hyperparameters completed below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa4a3e-23f3-4ac1-991c-5a47646f6509",
   "metadata": {},
   "source": [
    "### Experiment with NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d051ac2-a4f7-4865-be95-60911a3881cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 100\n",
    "activation     = 'LeakyReLU'\n",
    "num_epochs     = 100\n",
    "learning_rate  = 0.002\n",
    "minibatch_size = 64\n",
    "model_num      = 1\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912b4dc-1588-4318-a948-86ee95cdd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_exp = Sequential()\n",
    "\n",
    "NN_model_exp.add(Dense(n_neuron,  activation=activation, name='hidden_layer_1', input_shape=(X_train.shape[1],))) #  the 1st hidden layer \n",
    "NN_model_exp.add(Dense(n_neuron,  activation=activation, name='hidden_layer_2')) # the 2nd hidden layer\n",
    "NN_model_exp.add(Dense(n_neuron,  activation=activation, name='hidden_layer_3')) # the 3rd hidden layer\n",
    "NN_model_exp.add(Dense(1,  activation='linear', name='output_layer')) # the output layer\n",
    "\n",
    "\n",
    "NN_model_exp.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92565b83-e320-4548-be7c-e164cd43462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_exp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b71bc-8792-44ae-a7e3-92ee7f068d75",
   "metadata": {},
   "source": [
    "### Train Experimental NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de066da5-5a8b-467e-b601-8977c7cfb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_exp = NN_model_exp.fit(X_train, y_train, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61593e4f-6574-42cc-81b8-81e5ed4a1f15",
   "metadata": {},
   "source": [
    "### Initial Checks on Experimental NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87fe7c9-45d6-4316-804c-f87923406937",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.plot_history(history_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3e983-53ff-4633-849a-d1075a6ce582",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_exp = NN_model_exp.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_exp)\n",
    "print(\"MSE: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42d44c-4996-4e27-a2a2-d1f818d32ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test))\n",
    "plt.plot(x_ax, y_test, label=\"original\")\n",
    "plt.plot(x_ax, y_pred_exp, label=\"predicted\")\n",
    "plt.title(\"pC02 test and predicted data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fde844-f886-4a12-abc9-373148ef0a46",
   "metadata": {},
   "source": [
    "### Save Best Experimental NN Model as First of NN Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729eb6e0-823e-4fd0-b073-f83c9135fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, save:\n",
    "NN_model_exp.save(os.path.join(recon_model_path,'NN_model1.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32d972-24be-4ee4-8739-1dd13f9bb76c",
   "metadata": {},
   "source": [
    "# Five Identical NN Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7b3aa-443f-4c9b-b2ce-c9438d72248b",
   "metadata": {},
   "source": [
    "## NN Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13760f3-87b7-4524-94a9-2d535f867203",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model1 = load_model(os.path.join(recon_model_path,'NN_model1.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc2337-b817-4505-838a-0d9c23c3227f",
   "metadata": {},
   "source": [
    "## NN Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459c3b5-d216-453c-b1bb-1e6ab77f05b6",
   "metadata": {},
   "source": [
    "## NN Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771784b2-0638-461f-a280-da513a240615",
   "metadata": {},
   "source": [
    "## NN Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f4594-bee7-4179-8986-59548cc9b7e7",
   "metadata": {},
   "source": [
    "## NN Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf8d72-469b-4897-a1ed-bc7d568cc0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fromML",
   "language": "python",
   "name": "fromml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
