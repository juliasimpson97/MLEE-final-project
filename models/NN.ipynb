{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00743f4-e104-4250-abb9-151a089163b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e0f753-5052-4522-bc07-40f3a98a9415",
   "metadata": {},
   "source": [
    "(to compare pre-transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10741bee-ac28-47e8-a90a-7a4191fb9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 11:44:31.354911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Machine learning libraries\n",
    "import sklearn            # machine-learning libary with many algorithms implemented\n",
    "#import xgboost as xgb     # extreme gradient boosting (XGB)\n",
    "#from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "# Python file with supporting functions\n",
    "import model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bc6f61-11c2-4a81-9e0b-ae575dabc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_model_path = '/home/julias/MLEE-final-project/models/saved_models/recon_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467bbcd-f2f3-4023-b331-ea64c2da9554",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Split Datasets and Create Versions for Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fcffe-5a30-41ac-b254-cc70942693c6",
   "metadata": {},
   "source": [
    "## Load Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4fc0d1a-3d0e-4fad-9519-e3a06fe51edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ds = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/X.nc').sortby(['time','xlon','ylat']).transpose('time','ylat','xlon')\n",
    "y_ds = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/y.nc').sortby(['time','xlon','ylat']).transpose('time','ylat','xlon')\n",
    "X_train_ds = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/X_train.nc').sortby(['time','xlon','ylat']).transpose('time','ylat','xlon')\n",
    "y_train_ds = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/y_train.nc').sortby(['time','xlon','ylat']).transpose('time','ylat','xlon')\n",
    "X_test_ds = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/X_test.nc').sortby(['time','xlon','ylat']).transpose('time','ylat','xlon')\n",
    "y_test_ds = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/y_test.nc').sortby(['time','xlon','ylat']).transpose('time','ylat','xlon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f4c3efff-9cd7-41fc-a846-0866a66fdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = X_ds.to_dataframe().dropna()\n",
    "y_df = y_ds.to_dataframe().dropna() \n",
    "X_train_df = X_train_ds.to_dataframe().dropna() \n",
    "y_train_df = y_train_ds.to_dataframe().dropna()\n",
    "X_test_df = X_test_ds.to_dataframe().dropna() \n",
    "y_test_df = y_test_ds.to_dataframe().dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce4e78ca-c8e0-45c3-95da-8e4de0831d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_ds.SSS[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd035843-d322-4ec1-8f1c-10e180fb904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for future for loop to open datasets\n",
    "#ds_name = {'X', 'y', 'X_train', 'y_train', 'X_test', 'y_test'}\n",
    "#ds_list = {'X_ds', 'y_ds', 'X_train_ds', 'y_train_ds', 'X_test_ds', 'y_test_ds'}\n",
    "\n",
    "#ddict = {'X': 'X_ds', \n",
    "#         'y': 'y_ds', \n",
    "#         'X_train': 'X_train_ds', \n",
    "#         'y_train': 'y_train_ds',\n",
    "#         'X_test': 'X_test_ds',\n",
    "#         'y_test': 'y_test_ds',\n",
    "#        }\n",
    "#data_path = '/home/julias/MLEE-final-project/proc_data/split_datasets/{}.nc'\n",
    "\n",
    "#for key in ddict:\n",
    "#    #print(key, 'corresponds to', ddict[key])\n",
    "#    ddict[key] = xr.open_dataset(data_path.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b6e17d5-5e03-4413-8494-1bfd4183a51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/julias/.conda/envs/fromML/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1878: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "#X_ds_list = {'X_ds', 'X_train_ds', 'X_test_ds'}\n",
    "#y_ds_list = {'y_ds', 'y_train_ds', 'y_test_ds'}\n",
    "\n",
    "X_ds_norm = normalize_X_dataset(X_ds)\n",
    "X_train_ds_norm = normalize_X_dataset(X_train_ds)\n",
    "X_test_ds_norm = normalize_X_dataset(X_test_ds)\n",
    "\n",
    "y_ds_norm = normalize_y_dataset(y_ds)\n",
    "y_train_ds_norm = normalize_y_dataset(y_train_ds)\n",
    "y_test_ds_norm = normalize_y_dataset(y_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb89329f-e6a7-47be-98b2-47999abfbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_norm = X_ds_norm.to_dataframe().dropna()\n",
    "X_train_df_norm = X_train_ds_norm.to_dataframe().dropna()\n",
    "X_test_df_norm = X_test_ds_norm.to_dataframe().dropna()\n",
    "y_df_norm = y_ds_norm.to_dataframe().dropna()\n",
    "y_train_df_norm = y_train_ds_norm.to_dataframe().dropna()\n",
    "y_test_df_norm = y_test_ds_norm.to_dataframe().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba546f7-3a00-4c7a-b63d-8521d7b188c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Numpy Arrays for Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d9970-c39c-418b-ad4f-fd875a207756",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = X_df.to_numpy()         \n",
    "y_original = y_df.to_numpy().ravel() \n",
    "X_train_original = X_train_df.to_numpy() \n",
    "y_train_original = y_train_df.to_numpy().ravel()\n",
    "X_test_original = X_test_df.to_numpy()  \n",
    "y_test_original = y_test_df.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3184119-5595-4ac1-8982-c44a75113b6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Normalized Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871a31f-0a92-4103-91e0-752453105307",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## First Method of Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98239f49-9280-4cf4-8b9a-4c7cf49664fd",
   "metadata": {},
   "source": [
    "*Note: Done here, as opposed to in test/train split, so that I can save the original train/test datasets and later determine whether normalization led to improvement. (My group has not historically normalized data before training.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060b9f6-3b19-49a1-af35-599546707aab",
   "metadata": {},
   "source": [
    "ALSO note, done after the above test on original dataframes because method overwrites original dataframes, need to debug this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecee45b-abdc-42e3-a85f-bb2f6500b2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_df_norm = (X_df - X_df.mean())/X_df.std() \n",
    "#y_df_norm = (y_df - y_df.mean())/y_df.std()\n",
    "#X_train_df_norm = (X_train_df - X_train_df.mean())/X_train_df.std()\n",
    "#y_train_df_norm = (y_train_df - y_train_df.mean())/y_train_df.std()\n",
    "#X_test_df_norm = (X_test_df - X_test_df.mean())/X_test_df.std()\n",
    "#y_test_df_norm = (y_test_df - y_test_df.mean())/y_test_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d49cf6-81cc-4b9e-987a-832ad83b571c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Second Method of Normalization (Previously Implemented and Saved) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a44e58-fe0c-4e2b-ae5f-48f7ebd3e59b",
   "metadata": {},
   "source": [
    "- This way, time and lat/lon conversions are not normalized\n",
    "- Note that coordinates don't seem to be input into ML algorithms, so T0,T1,A,B,C are the inputs of time and space (as would want, don't want two forms of time and space input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777794ce-51e9-4ad8-869e-1a88fbe4e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_df_norm = X_df \n",
    "#y_df_norm = y_df \n",
    "#X_train_df_norm = X_train_df \n",
    "#y_train_df_norm = y_train_df  \n",
    "#X_test_df_norm = X_test_df  \n",
    "#y_test_df_norm = y_test_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ff75f-4674-436d-8c3b-5583defcd3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for df in X_df_norm, X_train_df_norm, X_test_df_norm:\n",
    "#    df.SSS = (df.SSS - df.SSS.mean())/df.SSS.std() \n",
    "#    df.SST = (df.SST - df.SST.mean())/df.SST.std() \n",
    "#    df.MLD = (df.MLD - df.MLD.mean())/df.MLD.std() \n",
    "#    df.Chl = (df.Chl - df.Chl.mean())/df.Chl.std() \n",
    "#    df.XCO2 = (df.XCO2 - df.XCO2.mean())/df.XCO2.std() \n",
    "\n",
    "#for df in y_df_norm, y_train_df_norm, y_test_df_norm:\n",
    "#    df.pCO2 = (df.pCO2 - df.pCO2.mean())/df.pCO2.std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda086a0-3f93-48f8-a473-dfe7204f8e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b08bb-f4f5-4ba5-8eb9-5ab8c85e6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b73140-80a7-4b39-92a8-7358f18fa4c6",
   "metadata": {},
   "source": [
    "Recreate original dataframes, as they are getting overwritten by the above process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2fc0f1-68f2-4f6b-8654-0da9fd58b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/X.nc').to_dataframe().dropna()\n",
    "#y_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/y.nc').to_dataframe().dropna() \n",
    "#X_train_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/X_train.nc').to_dataframe().dropna() \n",
    "#y_train_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/y_train.nc').to_dataframe().dropna()\n",
    "#X_test_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/X_test.nc').to_dataframe().dropna() \n",
    "#y_test_df = xr.open_dataset('/home/julias/MLEE-final-project/proc_data/split_datasets/y_test.nc').to_dataframe().dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b4685-25b6-47e6-8ad7-274e1e23f3a5",
   "metadata": {},
   "source": [
    "## Third (Updated) Method of Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8448d54-d758-47a1-b697-37f8ad8c5877",
   "metadata": {},
   "source": [
    "- Surprised at lack of improvement from normalization (as done below using the second method above)  \n",
    "- Problem may be coming from the use of a global mean/std, as opposed to one specific to each latitude and longitude point. \n",
    "- Replaced normalized numpy arrays and dataframes previously run and saved (names normdf, normdf2, normnumpy, and normnumpy2) with updated ones for each coordinate point (names normdfupdated and normnumpyupdated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d239bf1-164e-44cf-8b9f-e98c4e8f1905",
   "metadata": {},
   "source": [
    "## Create Numpy Arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e8d54-4cea-4f53-8ad2-6141f0f8d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n = X_df_norm.to_numpy()         \n",
    "y_n = y_df_norm.to_numpy().ravel() \n",
    "X_train_n = X_train_df_norm.to_numpy()\n",
    "y_train_n = y_train_df_norm.to_numpy().ravel()\n",
    "X_test_n = X_test_df_norm.to_numpy() \n",
    "y_test_n = y_test_df_norm.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb795547-8c7a-440c-8978-ed8fc4fe3f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0fedb-ef98-4547-b998-3afbda7c40a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce4a03-f5e8-4041-be7d-c9d7965858f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3de67bf9-7fdb-433b-8f49-04798e3232e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preliminary NN Model using Original (Not Normalized) Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e08329-b782-4318-847b-0234a2535769",
   "metadata": {},
   "source": [
    "## Build NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3285a-dfbc-4cfb-84fc-cd6059f7ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that number of input layer neurons must correspond to number of predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c7b34-a656-4973-8a0f-c50bddb5711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 64\n",
    "activation     = 'LeakyReLU'\n",
    "num_epochs     = 50\n",
    "learning_rate  = 0.001\n",
    "minibatch_size = 64\n",
    "model_num      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf97663-ae3c-4382-97d8-83bfe6515261",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d6bf4-9a85-4007-8aea-ef6af8541e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "NN_model.add(Dense(n_neuron, name='hidden_layer_1', activation=activation,input_shape=(X_train_df.shape[1],))) #  the 1st hidden layer \n",
    "NN_model.add(Dense(n_neuron, name='hidden_layer_2', activation=activation)) # the 2nd hidden layer\n",
    "NN_model.add(Dense(n_neuron, name='hidden_layer_3', activation=activation)) # the 3rd hidden layer\n",
    "NN_model.add(Dense(1, name='output_layer', activation='linear')) # the output layer\n",
    "\n",
    "\n",
    "NN_model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02822248-ca18-409d-93ff-bb2ffb7cfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e5ff7-d997-44eb-94ce-af0e37e51901",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "\n",
    "history = NN_model.fit(X_train_df, y_train_df, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5843abae-50f3-4b33-b9db-159af1f0a68e",
   "metadata": {},
   "source": [
    "## Initial Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee3774-5f25-4e80-81d9-c420444c052a",
   "metadata": {},
   "source": [
    "Can see that validation loss reaches minimums and jumps around, plot to see if trend indicates overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ef8f7-3b30-4721-86a5-445039ff4774",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.plot_history(history)\n",
    "plt.title('Preliminary NN with Original Dataframe Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce96e48-ff81-409c-abe2-ec5d06680489",
   "metadata": {},
   "source": [
    "- Indicates that the model isn't generalizing well\n",
    "- While the training loss goes down substantially, the validation loss does not show the same trend. Instead, it remains (relatively) high and jumps around.\n",
    "    - Completely fine, just signals that hyperparameter tuning will be needed (as expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdf458-c037-4f8f-8325-0bf6b901fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize so more comparable to other plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe75f2e-66a9-4bad-bfe8-77299a2bc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9771e-9445-4dc2-96aa-99e79af1dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_loss = (history.history['loss'] - np.mean(history.history['loss'])) / np.std(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9dddf-6b24-4a24-baeb-d5480e018d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_val_loss = (history.history['val_loss'] - np.mean(history.history['val_loss'])) / np.std(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23f6d5-334b-4e46-b430-d11bd79ce3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_ax = history.epoch\n",
    "#plt.plot(x_ax, norm_loss, label=\"training\")\n",
    "#plt.plot(x_ax, norm_val_loss, label=\"validation\")\n",
    "#plt.title(\"Normalized Training and Validation Loss over Epochs\")\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1d5c5-879f-48f2-a86e-6d67008a584c",
   "metadata": {},
   "source": [
    "## Save NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11fdfc-ff6a-406e-9e2c-d9f5dc602a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, save:\n",
    "NN_model.save(os.path.join(recon_model_path,'NN_model_prelim_originaldf2.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a61ba-79d8-4b49-aad8-2ae339817221",
   "metadata": {},
   "source": [
    "## Test NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcbadfc-c85d-4ef8-bbaa-de43baf89406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then reload before start working with test data\n",
    "NN_prelim_model = load_model(os.path.join(recon_model_path,'NN_model_prelim_originaldf2.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee887813-7c03-4130-a95b-87758a3e5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN_prelim_model.predict(X_test_df)\n",
    "mse = mean_squared_error(y_test_df, y_pred)\n",
    "print(\"MSE: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b17020-f0ed-4ded-abb4-054328118aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test_df))\n",
    "plt.plot(x_ax, y_test_df, label=\"original\")\n",
    "plt.plot(x_ax, y_pred, label=\"predicted\")\n",
    "plt.title(\"pC02 Test and Predicted Data for Preliminary NN with Original Dataframe Input\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a63253-e16d-4233-9e6e-ee67aaa14c9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preliminary NN Model using Original Numpy Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579870a-7bad-45dc-87fa-0b2910156cb9",
   "metadata": {},
   "source": [
    "## Build NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526d3e4-5630-4eef-887d-b7fd4ae7cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 64\n",
    "activation     = 'LeakyReLU'\n",
    "num_epochs     = 50\n",
    "learning_rate  = 0.001\n",
    "minibatch_size = 64\n",
    "model_num      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b3712-be27-4f75-8fb0-afdb221811af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd50301-8638-4168-9f7f-d1bac1f0b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_on = Sequential()\n",
    "\n",
    "NN_model_on.add(Dense(n_neuron,  activation=activation,input_shape=(X_train_original.shape[1],))) #  the 1st hidden layer \n",
    "NN_model_on.add(Dense(n_neuron,  activation=activation)) # the 2nd hidden layer\n",
    "NN_model_on.add(Dense(n_neuron,  activation=activation)) # the 3rd hidden layer\n",
    "NN_model_on.add(Dense(1,  activation='linear')) # the output layer\n",
    "\n",
    "\n",
    "NN_model_on.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a26af-4be5-425d-a30a-7e28fb8742b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_on.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742246d-2fc7-4033-b839-7e9222b6e204",
   "metadata": {},
   "source": [
    "## Train NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e60e27-a5cc-4d98-97b5-e5cac65ea233",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "\n",
    "history_on = NN_model_on.fit(X_train_original, y_train_original, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28472c8-7daa-459d-8d4b-97cae8bd9a4f",
   "metadata": {},
   "source": [
    "## Initial Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890c796-7e92-49cc-8dbf-d7b39916bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.plot_history(history_on)\n",
    "plt.title('Preliminary NN with Original Numpy Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f49d5-0013-4918-9ecd-f411f47330dd",
   "metadata": {},
   "source": [
    "## Save NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466c3a2-4784-4e25-bf35-0cb311011666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, save:\n",
    "NN_model_on.save(os.path.join(recon_model_path,'NN_model_prelim_originalnumpy2.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d9d50-9b48-40a4-8a4d-a9e608bda47e",
   "metadata": {},
   "source": [
    "## Test NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaace4d-aecf-430c-9b79-6658806b5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_on = load_model(os.path.join(recon_model_path,'NN_model_prelim_originalnumpy2.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ee9e8-f69b-4be2-b5e0-2321d9b95934",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_on = NN_model_on.predict(X_test_original)\n",
    "mse = mean_squared_error(y_test_original, y_pred_on)\n",
    "print(\"MSE: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a2356-3b97-4c36-a600-665f3126c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test_original))\n",
    "plt.plot(x_ax, y_test_original, label=\"original\")\n",
    "plt.plot(x_ax, y_pred_on, label=\"predicted\")\n",
    "plt.title(\"pC02 Test and Predicted Data for Preliminary NN with Original Numpy Input\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0238b1-8c3a-46fb-8726-7d157ea07066",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preliminary NN Model using Normalized Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d07a232-3847-4085-ac2b-a8299dc52b8c",
   "metadata": {},
   "source": [
    "## Build NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84323fd7-e2d9-4157-8bea-14299df57876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 64\n",
    "activation     = 'LeakyReLU'\n",
    "num_epochs     = 50\n",
    "learning_rate  = 0.001\n",
    "minibatch_size = 64\n",
    "model_num      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d3b5b-cf4f-45b3-b99b-2593d468a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_norm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ce14a-d99b-421b-aeb1-e4300596c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_nprelim = Sequential()\n",
    "\n",
    "NN_model_nprelim.add(Dense(n_neuron,  activation=activation,input_shape=(X_train_df_norm.shape[1],))) #  the 1st hidden layer \n",
    "NN_model_nprelim.add(Dense(n_neuron,  activation=activation)) # the 2nd hidden layer\n",
    "NN_model_nprelim.add(Dense(n_neuron,  activation=activation)) # the 3rd hidden layer\n",
    "NN_model_nprelim.add(Dense(1,  activation='linear')) # the output layer\n",
    "\n",
    "\n",
    "NN_model_nprelim.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415335b8-85d1-466d-b8c1-eb1f7cd4edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_nprelim.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf7a2a-12a0-448c-8173-bdfc581669a4",
   "metadata": {},
   "source": [
    "## Train NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0bc27-0d98-433b-b157-248b247f3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "\n",
    "history_nprelim = NN_model_nprelim.fit(X_train_df_norm, y_train_df_norm, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d336b5-8489-40fc-b99f-2dbfa4e938c4",
   "metadata": {},
   "source": [
    "## Initial Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0311032-0eb4-417a-8fbc-6b635ea81613",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.plot_history(history_nprelim)\n",
    "plt.title('Preliminary NN with Normalized Dataframe Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be0e9f-eeaa-43cb-bb7a-c5b0db7e55d5",
   "metadata": {},
   "source": [
    "## Save NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8e74d-fc66-4cdf-aa39-04663b8b456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, save:\n",
    "NN_model_nprelim.save(os.path.join(recon_model_path,'NN_model_prelim_normdfupdated.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a12c36-5dc7-4419-846d-32f1eb6eff74",
   "metadata": {},
   "source": [
    "## Test NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0986fca-abd8-4005-bb0c-38d1d6fcc430",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_nprelim = load_model(os.path.join(recon_model_path,'NN_model_prelim_normdfupdated.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7b3e7-d287-4149-b802-390d562ce5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nprelim = NN_model_nprelim.predict(X_test_df_norm)\n",
    "mse = mean_squared_error(y_test_df_norm, y_pred_nprelim)\n",
    "print(\"MSE: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6783f-40f5-4f2b-84fd-613147f0a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test_df_norm))\n",
    "plt.plot(x_ax, y_test_df_norm, label=\"original\")\n",
    "plt.plot(x_ax, y_pred_nprelim, label=\"predicted\")\n",
    "plt.title(\"pC02 Test and Predicted Data for Preliminary NN with Normalized Dataframe Input\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe42f69-d3ea-4dbd-a866-7cc25d6c7b7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preliminary NN Model using Normalized Numpy Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8854d-3069-44a5-90a8-dc8debcc3b70",
   "metadata": {},
   "source": [
    "## Build NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0b7f9-7fd2-4664-85a6-86e94dead6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 64\n",
    "activation     = 'LeakyReLU'\n",
    "num_epochs     = 50\n",
    "learning_rate  = 0.001\n",
    "minibatch_size = 64\n",
    "model_num      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bf920-9400-4ad6-842f-03d0f4abebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75775626-3382-46c9-8b1f-e1598f04f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_nn = Sequential()\n",
    "\n",
    "NN_model_nn.add(Dense(n_neuron,  activation=activation,input_shape=(X_train_n.shape[1],))) #  the 1st hidden layer \n",
    "NN_model_nn.add(Dense(n_neuron,  activation=activation)) # the 2nd hidden layer\n",
    "NN_model_nn.add(Dense(n_neuron,  activation=activation)) # the 3rd hidden layer\n",
    "NN_model_nn.add(Dense(1,  activation='linear')) # the output layer\n",
    "\n",
    "\n",
    "NN_model_nn.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3005dd-752a-4220-bccc-200c3268454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22961bb1-7fd5-4b4b-b38c-c1684a7e566a",
   "metadata": {},
   "source": [
    "## Train NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c92bef-cf1f-4ae9-a626-9133fbda937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "\n",
    "history_nn = NN_model_nn.fit(X_train_n, y_train_n, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61c8d8-c47a-4d3e-bb89-4c3362d1bab6",
   "metadata": {},
   "source": [
    "## Initial Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff2061-24f1-467c-9e17-0b56967fca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.plot_history(history_nn)\n",
    "plt.title('Preliminary NN with Normalized Numpy Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692e08a-0710-41a3-a513-e496dc7ba257",
   "metadata": {},
   "source": [
    "## Save NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f97e1-fdb9-4938-a9f8-1cf3fe340c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, save:\n",
    "NN_model_nn.save(os.path.join(recon_model_path,'NN_model_prelim_normnumpyupdated.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf2c89-3c9d-445d-80da-f4c162b06dd8",
   "metadata": {},
   "source": [
    "## Test NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03c361-382f-468c-ab3b-89ee0ce14b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_nn = load_model(os.path.join(recon_model_path,'NN_model_prelim_normnumpyupdated.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655b694-a5a0-4a64-8668-3e776958adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = NN_model_nn.predict(X_test_n)\n",
    "mse = mean_squared_error(y_test_n, y_pred_nn)\n",
    "print(\"MSE: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84413bc-4d6f-4ada-94b2-e71e25a84885",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test_n))\n",
    "plt.plot(x_ax, y_test_n, label=\"original\")\n",
    "plt.plot(x_ax, y_pred_nn, label=\"predicted\")\n",
    "plt.title(\"pC02 Test and Predicted Data for Preliminary NN with Normalized Numpy Input\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15331af5-9672-417c-be3c-0870d4707f1f",
   "metadata": {},
   "source": [
    "# Select Approach & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c304571-5689-466a-957e-29a6559d6b72",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65976d40-1aac-4452-9520-a8778d7df3d7",
   "metadata": {},
   "source": [
    "- Based on preliminary experimentation, will train the NN model on original dataframes\n",
    "- Comparison completed above for group reference, as data has not historically been normalized prior to training\n",
    "- Regarding normalization\n",
    "    - Likely need to debug or investigate normalizing specific input parameters\n",
    "    - For both the dataframe and numpy normalized inputs, validation losses did not decrease with training losses. This could mean overfitting to the training set; maybe normalization makes it \"too easy\" for the algorithm to learn the training set.\n",
    "- Regarding dataframe vs numpy array\n",
    "    - The difference between dataframe and numpy input is likely not significant, as runs for original and normalized data were similar. \n",
    "    - NN based on original dataframe input had a slightly higher MSE than that based on the numpy input, but the difference likely was not significant/could be due to random variation in runs. \n",
    "    - Visually, use of the original dataframe resulted in more closely aligned predicted pCO2, particularly at the extremes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606d841-9f96-4c1e-984c-f24508117dc3",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b4107-3bea-4c70-b399-1a0a3043f17b",
   "metadata": {},
   "source": [
    "Experimentation with hyperparameters completed below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa4a3e-23f3-4ac1-991c-5a47646f6509",
   "metadata": {},
   "source": [
    "### Experiment with NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d051ac2-a4f7-4865-be95-60911a3881cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Run\n",
    "#n_neuron       = 100\n",
    "#activation     = 'LeakyReLU'\n",
    "#num_epochs     = 100\n",
    "#learning_rate  = 0.002\n",
    "#minibatch_size = 64\n",
    "#model_num      = 1\n",
    "#early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# loss: 124.3420 - val_loss: 289.1165\n",
    "# MSE: 181.18\n",
    "# stopped after 91 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537272d1-0f4d-4ea0-8c6f-27b74c10a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Run\n",
    "#n_neuron       = 64\n",
    "#activation     = 'LeakyReLU'\n",
    "#num_epochs     = 100\n",
    "#learning_rate  = 0.0015\n",
    "#minibatch_size = 64\n",
    "#model_num      = 1\n",
    "#early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "#NN_model_exp.add(Dense(n_neuron, activation=activation, name='hidden_layer_4')) # added a 4th layer\n",
    "\n",
    "# loss: mid 100s - val_loss: mid 300s\n",
    "# MSE: 228.09\n",
    "# stopped after 49 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2cf29-0eec-4124-b829-24ded9d53355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Run\n",
    "#n_neuron       = 64\n",
    "#activation     = 'LeakyReLU'\n",
    "#num_epochs     = 100\n",
    "#learning_rate  = 0.0005\n",
    "#minibatch_size = 64\n",
    "#model_num      = 1\n",
    "#early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "#NN_model_exp.add(Dense(n_neuron, activation=activation, name='hidden_layer_4')) # added a 4th layer\n",
    "\n",
    "#loss: 193.3637 - val_loss: 375.5946\n",
    "# MSE:\n",
    "# stopped after 40 epochs, but at start was much more aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763b189-40a2-440e-8c92-6d279f51b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th and 5th runs\n",
    "#n_neuron       = 64\n",
    "#activation     = 'ReLU'\n",
    "#num_epochs     = 100\n",
    "#learning_rate  = 0.0005\n",
    "#minibatch_size = 64\n",
    "#model_num      = 1\n",
    "# and made only two hidden layers \n",
    "#early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# MSE: 289.26, loss: 257.6627 - val_loss: 366.7357 but trend MUCH more aligned, up patience from 10 to 20, and try Relu\n",
    "\n",
    "# MSE: 267.70, loss: 225.0135 - val_loss: 318.7133, trend aligned even more, through almost all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3148f4e-8ba4-44c2-944a-6ba9c770b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th run\n",
    "#n_neuron       = 100\n",
    "#activation     = 'ReLU'\n",
    "#num_epochs     = 100\n",
    "#learning_rate  = 0.0008\n",
    "#minibatch_size = 64\n",
    "#model_num      = 1\n",
    "# and made only two hidden layers \n",
    "#early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# ran for 74 epochs\n",
    "# MSE: 255.24, loss: 180.7052 - val_loss: 294.3801 \n",
    "# train/val trend not quite as aligned as previous run\n",
    "# go back to first run and start again from there\n",
    "# common between first run and this one: higher number of neurons, up again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace5e74-ccf0-4fe3-b7aa-c3f0a969bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th Run and 8th Run\n",
    "#n_neuron       = 150\n",
    "#activation     = 'ReLU'\n",
    "#num_epochs     = 100\n",
    "#learning_rate  = 0.002\n",
    "#minibatch_size = 64\n",
    "#model_num      = 1\n",
    "#early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# loss: 178.2955 - val_loss: 316.7465\n",
    "# definitely not, trend not great again --> try with Relu instead of Leaky \n",
    "# MSE: 219.80, 162.3405 - val_loss: 343.4025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dbe4cb-c96e-4ca9-9138-8b4c495780eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9th Run\n",
    "#n_neuron       = 100\n",
    "#activation     = 'ReLU'\n",
    "#num_epochs     = 100\n",
    "#learning_rate  = 0.002\n",
    "#minibatch_size = 64\n",
    "#model_num      = 1\n",
    "#early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# MSE: 208.60, loss: 170.7158 - val_loss: 282.2546\n",
    "# good train/val loss alignment but stopped at 64 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4ff96-52a1-4bd1-827c-d1778f8769d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10th Run\n",
    "#n_neuron       = 100\n",
    "#activation     = 'ReLU'\n",
    "#num_epochs     = 100\n",
    "#learning_rate  = 0.0005\n",
    "#minibatch_size = 100\n",
    "#model_num      = 1\n",
    "#early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "#loss: 186.7061 - val_loss: 267.3068, MSE: 238.85\n",
    "# but, best alignment between train and val loss so far\n",
    "# take best run so far and increase batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea8d1f-85c8-480e-81d6-37326990308b",
   "metadata": {},
   "source": [
    "Take best trends of all runs\n",
    "- Relu over LeakyRelu\n",
    "- Increase mini batch size to 100\n",
    "- Don't alter neurons (worse when increase and decrease)\n",
    "- Maintain learning rate, but maybe could go down, 0.0005 did well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1312b-2e96-4a3b-b526-9dcca1b7cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11th and 12th Run and 13th run **12th is saved, 13th just swapped back to Leaky Relu **was worse, back to run 12\n",
    "n_neuron       = 100\n",
    "activation     = 'ReLU'\n",
    "num_epochs     = 100\n",
    "learning_rate  = 0.0005 # 0.002 definitely overtrained, val didn't decrease after approx 20 epochs, loss: 144.7867 - val_loss: 389.3859, MSE 208\n",
    "minibatch_size = 100\n",
    "model_num      = 1\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# loss: 143.5514 - val_loss: 239.9906\n",
    "# MSE: 192.47\n",
    "# ran all 100 epochs, and really consistent train val losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912b4dc-1588-4318-a948-86ee95cdd315",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_exp = Sequential()\n",
    "\n",
    "NN_model_exp.add(Dense(n_neuron, activation=activation, name='hidden_layer_1', input_shape=(X_train_df.shape[1],))) #  the 1st hidden layer \n",
    "NN_model_exp.add(Dense(n_neuron, activation=activation, name='hidden_layer_2')) # the 2nd hidden layer\n",
    "NN_model_exp.add(Dense(n_neuron, activation=activation, name='hidden_layer_3')) \n",
    "NN_model_exp.add(Dense(1, activation='linear', name='output_layer')) # the output layer\n",
    "\n",
    "\n",
    "NN_model_exp.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92565b83-e320-4548-be7c-e164cd43462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_exp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b71bc-8792-44ae-a7e3-92ee7f068d75",
   "metadata": {},
   "source": [
    "### Train Experimental NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de066da5-5a8b-467e-b601-8977c7cfb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_exp = NN_model_exp.fit(X_train_df, y_train_df, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61593e4f-6574-42cc-81b8-81e5ed4a1f15",
   "metadata": {},
   "source": [
    "### Initial Checks on Experimental NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87fe7c9-45d6-4316-804c-f87923406937",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.plot_history(history_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3e983-53ff-4633-849a-d1075a6ce582",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_exp = NN_model_exp.predict(X_test_df)\n",
    "mse = mean_squared_error(y_test_df, y_pred_exp)\n",
    "print(\"MSE: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42d44c-4996-4e27-a2a2-d1f818d32ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(len(y_test_df))\n",
    "plt.plot(x_ax, y_test_df, label=\"original\")\n",
    "plt.plot(x_ax, y_pred_exp, label=\"predicted\")\n",
    "plt.title(\"pC02 test and predicted data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fde844-f886-4a12-abc9-373148ef0a46",
   "metadata": {},
   "source": [
    "### Save Best Experimental NN Model as First of NN Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729eb6e0-823e-4fd0-b073-f83c9135fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, save:\n",
    "NN_model_exp.save(os.path.join(recon_model_path,'NN_model1.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32d972-24be-4ee4-8739-1dd13f9bb76c",
   "metadata": {},
   "source": [
    "# Five Identical NN Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7b3aa-443f-4c9b-b2ce-c9438d72248b",
   "metadata": {},
   "source": [
    "## NN Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13760f3-87b7-4524-94a9-2d535f867203",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model1 = load_model(os.path.join(recon_model_path,'NN_model1.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc2337-b817-4505-838a-0d9c23c3227f",
   "metadata": {},
   "source": [
    "## NN Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459c3b5-d216-453c-b1bb-1e6ab77f05b6",
   "metadata": {},
   "source": [
    "## NN Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771784b2-0638-461f-a280-da513a240615",
   "metadata": {},
   "source": [
    "## NN Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f4594-bee7-4179-8986-59548cc9b7e7",
   "metadata": {},
   "source": [
    "## NN Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf8d72-469b-4897-a1ed-bc7d568cc0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fromML",
   "language": "python",
   "name": "fromml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
